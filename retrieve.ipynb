{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manthan/Drive/Work/VSCode/LLMs/Real Time Retrievel System for Social Media/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils.embedding import EmbeddingModelSingleton, CrossEncoderModelSingleton\n",
    "from utils.qdrant import build_qdrant_client\n",
    "from utils.retriever import QdrantVectorDBRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "embedding_model = EmbeddingModelSingleton()\n",
    "cross_encode_model = CrossEncoderModelSingleton()\n",
    "qdrant_client = build_qdrant_client()\n",
    "\n",
    "vectordb_retriever = QdrantVectorDBRetriever(\n",
    "    embedding_model=embedding_model,\n",
    "    cross_encoder_model=cross_encode_model,\n",
    "    vector_db_client=qdrant_client\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "all_posts = vectordb_retriever.scroll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"font-family: Arial, sans-serif; color: black; margin: 10px; padding: 20px; border-radius: 10px; background-color: #f3f3f3; box-shadow: 0 0 10px rgba(0,0,0,0.1);\">\n",
       "            <h2 style=\"color: #333;\">Post ID: Post_16</h2>\n",
       "            <h3 style=\"color: #555;\">Chunk ID: 7a2c935a-b89f-61b6-76cb-151a3ae8e068</h3>\n",
       "            <p><strong>Embedding Length:</strong> 384</p>\n",
       "        <p><strong>Score:</strong> 0.70371795</p><p><strong>Rerank Score:</strong> -0.1416376531124115</p>\n",
       "            <p><strong>Chunked Text:</strong> \" real - time data source, bytewax to transform raw text into vector embeddings, and qdrant as a serverless vector db, to store and retrieve embeddings at inference time. you will find the link to the github repo in the comment section below what's next? in the next lecture, we will cover the inference pipeline, so our system can serve predictions to end users. thank you paul iusztin and alexandru rzvan for making this course possible. it is a pleasure working with you guys! - - - - hi there! it's pau labarta bajo every week i share free, hands - on content, on production - grade ml, to help you build real - world ml products. follow me and click on the so you don't miss what's coming next hashtag # machinelearning hashtag # mlops hashtag # realworldml hashtag # llmops hashtag # llms hashtag # bytewax hashtag # qdrant hashtag # vectordb hashtag # realtimeml \"</p>\n",
       "            <p><strong>Full Raw Text:</strong> \" ğŸ¬ ğ—Ÿğ—²ğ˜€ğ˜€ğ—¼ğ—» ğŸ¯ of the ğ—›ğ—®ğ—»ğ—±ğ˜€-ğ—¼ğ—» ğ—Ÿğ—Ÿğ—  ğ—°ğ—¼ğ˜‚ğ—¿ğ˜€ğ—² is ğ—¢ğ—¨ğ—§ ğŸ¬\n",
       "ğ—–ğ—¼ğ—»ğ˜ğ—²ğ˜…ğ˜\n",
       "LLMs are as good as the data you embed in your prompts. And for many real-world problems, this means the data needs to be both\n",
       "â†’ good, and\n",
       "â†’ fresh\n",
       "ğ—˜ğ˜…ğ—®ğ—ºğ—½ğ—¹ğ—² ğŸ’\n",
       "Imagine you build a great LLM-based financial advisorâ€¦ but you only feed it with outdated data.\n",
       "No matter how good your model is, the predictions it will generate will be rubbish ğŸ«£\n",
       "So, the question is\n",
       "ğ™ƒğ™¤ğ™¬ ğ™™ğ™¤ ğ™›ğ™šğ™šğ™™ ğ™›ğ™§ğ™šğ™¨ğ™ ğ™™ğ™–ğ™©ğ™– ğ™©ğ™¤ ğ™®ğ™¤ğ™ªğ™§ ğ™‡ğ™‡ğ™ˆ â“\n",
       "ğ—§ğ—µğ—² ğ˜€ğ—¼ğ—¹ğ˜‚ğ˜ğ—¶ğ—¼ğ—» ğŸ§ \n",
       "You need to build a ğ—¿ğ—²ğ—®ğ—¹-ğ˜ğ—¶ğ—ºğ—² ğ˜ğ—²ğ˜…ğ˜ ğ—²ğ—ºğ—¯ğ—²ğ—±ğ—±ğ—¶ğ—»ğ—´ ğ—½ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—², that\n",
       "â†’ ğ—œğ—»ğ—´ğ—²ğ˜€ğ˜ğ˜€ raw text from your data source, in real-time\n",
       "â†’ ğ—§ğ—¿ğ—®ğ—»ğ˜€ğ—³ğ—¼ğ—¿ğ—ºğ˜€ this raw text into vector embeddings, and\n",
       "â†’ ğ—¦ğ˜ğ—¼ğ—¿ğ—²ğ˜€ these embeddings in a VectorDB, so your LLM can fetch and use them for Retrieval Augmented Generation (RAG) at inference time.\n",
       "ğ—™ğ˜‚ğ—¹ğ—¹ ğ˜€ğ—¼ğ˜‚ğ—¿ğ—°ğ—² ğ—°ğ—¼ğ—±ğ—² ğŸ‘¨ğŸ»â€ğŸ’»\n",
       "In Lesson 3 of the ğ—›ğ—®ğ—»ğ—±ğ˜€-ğ—¼ğ—» ğ—Ÿğ—Ÿğ—  ğ—–ğ—¼ğ˜‚ğ—¿ğ˜€ğ—², you will find a full source code implementation of a text embedding pipeline, that uses:\n",
       "â†’\n",
       "Alpaca\n",
       "News API as our real-time data source,\n",
       "â†’\n",
       "Bytewax\n",
       "to transform raw text into vector embeddings, and\n",
       "â†’\n",
       "Qdrant\n",
       "as a Serverless Vector DB, to store and retrieve embeddings at inference time.\n",
       "You will find the link to the ğ—šğ—¶ğ˜ğ—›ğ˜‚ğ—¯ ğ—¿ğ—²ğ—½ğ—¼ in the comment section below â¬‡ï¸\n",
       "ğ—ªğ—µğ—®ğ˜'ğ˜€ ğ—»ğ—²ğ˜…ğ˜? â­ï¸\n",
       "In the next lecture, we will cover the ğ—¶ğ—»ğ—³ğ—²ğ—¿ğ—²ğ—»ğ—°ğ—² ğ—½ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—², so our system can serve predictions to end users. ğŸš€\n",
       "THANK YOU\n",
       "Paul Iusztin\n",
       "and\n",
       "Alexandru RÄƒzvanÈ› ğŸ‘‹\n",
       "for making this course possible. It is a pleasure working with you guys!\n",
       "----\n",
       "Hi there! It's\n",
       "Pau Labarta Bajo\n",
       "ğŸ‘‹\n",
       "Every week I share free, hands-on content, on production-grade ML, to help you build real-world ML products.\n",
       "ğ—™ğ—¼ğ—¹ğ—¹ğ—¼ğ˜„ ğ—ºğ—² and ğ—°ğ—¹ğ—¶ğ—°ğ—¸ ğ—¼ğ—» ğ˜ğ—µğ—² ğŸ”” so you don't miss what's coming next\n",
       "hashtag\n",
       "#\n",
       "machinelearning\n",
       "hashtag\n",
       "#\n",
       "mlops\n",
       "hashtag\n",
       "#\n",
       "realworldml\n",
       "hashtag\n",
       "#\n",
       "llmops\n",
       "hashtag\n",
       "#\n",
       "llms\n",
       "hashtag\n",
       "#\n",
       "bytewax\n",
       "hashtag\n",
       "#\n",
       "qdrant\n",
       "hashtag\n",
       "#\n",
       "vectordb\n",
       "hashtag\n",
       "#\n",
       "realtimeml \"</p>\n",
       "            \n",
       "        </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"font-family: Arial, sans-serif; color: black; margin: 10px; padding: 20px; border-radius: 10px; background-color: #f3f3f3; box-shadow: 0 0 10px rgba(0,0,0,0.1);\">\n",
       "            <h2 style=\"color: #333;\">Post ID: Post_30</h2>\n",
       "            <h3 style=\"color: #555;\">Chunk ID: 04baa0ee-ae34-6a18-2237-3b4320f114c9</h3>\n",
       "            <p><strong>Embedding Length:</strong> 384</p>\n",
       "        <p><strong>Score:</strong> 0.66816294</p><p><strong>Rerank Score:</strong> -1.2617921829223633</p>\n",
       "            <p><strong>Chunked Text:</strong> \" and real - time financial news ( e. g. alpaca ) - a stream processing engine ( e. g., bytewax - [ url ] ) - an encoder - only model for embedding the documents ( e. g., pick one from ` sentence - transformers ` ) - a vector db ( e. g., qdrant - [ url ] ) how does it work? on the feature pipeline side : 1. using bytewax, you ingest the financial news and clean them 2. you chunk the news documents and embed them 3. you insert the embedding of the docs along with their metadata ( e. g., the initial text, source _ url, etc. ) to qdrant on the inference pipeline side : 4. the user question is embedded ( using the same embedding model ) 5. using this embedding, you extract the top k most similar news documents from qdrant 6. along with the user question, you inject the necessary metadata from the extracted top k documents into the prompt template ( e. g., the text of documents & its source _ url ) 7. you pass the whole prompt to the llm for the \"</p>\n",
       "            <p><strong>Full Raw Text:</strong> \" ğ—¥ğ—”ğ—š: ğ˜„ğ—µğ—®ğ˜ problems does it solve, and ğ—µğ—¼ğ˜„ it's integrated into ğ—Ÿğ—Ÿğ— -ğ—½ğ—¼ğ˜„ğ—²ğ—¿ğ—²ğ—± ğ—®ğ—½ğ—½ğ—¹ğ—¶ğ—°ğ—®ğ˜ğ—¶ğ—¼ğ—»ğ˜€?\n",
       "Let's find out â†“\n",
       "RAG is a popular strategy when building LLMs to add external data to your prompt.\n",
       "=== ğ—£ğ—¿ğ—¼ğ—¯ğ—¹ğ—²ğ—º ===\n",
       "Working with LLMs has 3 main issues:\n",
       "1. The world moves fast\n",
       "An LLM learns an internal knowledge base. However, the issue is that its knowledge is limited to its training dataset.\n",
       "The world moves fast. New data flows on the internet every second. Thus, the model's knowledge base can quickly become obsolete.\n",
       "One solution is to fine-tune the model every minute or day...\n",
       "If you have some billions to spend around, go for it.\n",
       "2. Hallucinations\n",
       "An LLM  is full of testosterone and likes to be blindly confident.\n",
       "Even if the answer looks 100% legit, you can never fully trust it.\n",
       "3. Lack of reference links\n",
       "It is hard to trust the response of the LLM if we can't see the source of its decisions.\n",
       "Especially for important decisions (e.g., health, financials)\n",
       "=== ğ—¦ğ—¼ğ—¹ğ˜‚ğ˜ğ—¶ğ—¼ğ—» ===\n",
       "â†’ Surprize! It is RAG.\n",
       "1. Avoid fine-tuning\n",
       "Using RAG, you use the LLM as a reasoning engine and the external knowledge base as the main memory (e.g., vector DB).\n",
       "The memory is volatile, so you can quickly introduce or remove data.\n",
       "2. Avoid hallucinations\n",
       "By forcing the LLM to answer solely based on the given context, the LLM will provide an answer as follows:\n",
       "-  use the external data to respond to the user's question if it contains the necessary insights\n",
       "- \"I don't know\" if not\n",
       "3. Add reference links\n",
       "Using RAG, you can easily track the source of the data and highlight it to the user.\n",
       "=== ğ—›ğ—¼ğ˜„ ğ—±ğ—¼ğ—²ğ˜€ ğ—¥ğ—”ğ—š ğ˜„ğ—¼ğ—¿ğ—¸? ===\n",
       "Let's say we want to use RAG to build a financial assistant.\n",
       "ğ˜ğ˜©ğ˜¢ğ˜µ ğ˜¥ğ˜° ğ˜¸ğ˜¦ ğ˜¯ğ˜¦ğ˜¦ğ˜¥?\n",
       "- a data source with historical and real-time financial news (e.g. Alpaca)\n",
       "- a stream processing engine (e.g., Bytewax - ğŸ”—\n",
       "https://lnkd.in/dWJytkZ5\n",
       ")\n",
       "- an encoder-only model for embedding the documents (e.g., pick one from `sentence-transformers`)\n",
       "- a vector DB (e.g., Qdrant - ğŸ”—\n",
       "https://lnkd.in/d_FA9Bb3\n",
       ")\n",
       "ğ˜ğ˜°ğ˜¸ ğ˜¥ğ˜°ğ˜¦ğ˜´ ğ˜ªğ˜µ ğ˜¸ğ˜°ğ˜³ğ˜¬?\n",
       "â†³ On the feature pipeline side:\n",
       "1. using Bytewax, you ingest the financial news and clean them\n",
       "2. you chunk the news documents and embed them\n",
       "3. you insert the embedding of the docs along with their metadata (e.g., the initial text, source_url, etc.) to Qdrant\n",
       "â†³ On the inference pipeline side:\n",
       "4. the user question is embedded (using the same embedding model)\n",
       "5. using this embedding, you extract the top K most similar news documents from Qdrant\n",
       "6. along with the user question, you inject the necessary metadata from the extracted top K documents into the prompt template (e.g., the text of documents & its source_url)\n",
       "7. you pass the whole prompt to the LLM for the final answer\n",
       "hashtag\n",
       "#\n",
       "machinelearning\n",
       "hashtag\n",
       "#\n",
       "mlops\n",
       "hashtag\n",
       "#\n",
       "datascience \"</p>\n",
       "            \n",
       "        </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"font-family: Arial, sans-serif; color: black; margin: 10px; padding: 20px; border-radius: 10px; background-color: #f3f3f3; box-shadow: 0 0 10px rgba(0,0,0,0.1);\">\n",
       "            <h2 style=\"color: #333;\">Post ID: Post_36</h2>\n",
       "            <h3 style=\"color: #555;\">Chunk ID: 6119e826-c23b-0181-8cf3-016680db83e5</h3>\n",
       "            <p><strong>Embedding Length:</strong> 384</p>\n",
       "        <p><strong>Score:</strong> 0.6675552</p><p><strong>Rerank Score:</strong> -2.5309348106384277</p>\n",
       "            <p><strong>Chunked Text:</strong> \" to the llm. 7. insert the user question + assistant answer to the chat history.. but the question is, how do you keep your vector db up to date with the latest data? you need a real - time streaming pipeline. how do you implement it? you need 2 components : a streaming processing framework. for example, bytewax is built in rust for efficiency and exposes a python interface for ease of use - you don't need java to implement real - time pipelines anymore. bytewax : [ url ] a vector db. for example, qdrant provides a rich set of features and a seamless experience. qdrant : [ url ]. here is an example of how to implement a streaming pipeline for financial news 1. financial news data source ( e. g., alpaca ) : to populate your vector db, you need a historical api ( e. g., restful api ) to add data to your vector db in batch mode between a desired [ start _ date, end _ date ] range. you can tweak the number of workers to parallelize this step as much as possible. you run this once in the beginning. you need the data exposed under a web socket to \"</p>\n",
       "            <p><strong>Full Raw Text:</strong> \" To successfully use ğ—¥ğ—”ğ—š in your ğ—Ÿğ—Ÿğ—  ğ—®ğ—½ğ—½ğ—¹ğ—¶ğ—°ğ—®ğ˜ğ—¶ğ—¼ğ—»ğ˜€, your ğ˜ƒğ—²ğ—°ğ˜ğ—¼ğ—¿ ğ——ğ—• must constantly be updated with the latest data.\n",
       "Here is how you can implement a ğ˜€ğ˜ğ—¿ğ—²ğ—®ğ—ºğ—¶ğ—»ğ—´ ğ—½ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—² to keep your vector DB in sync with your datasets â†“\n",
       ".\n",
       "ğ—¥ğ—”ğ—š is a popular strategy when building LLMs to add context to your prompt about your private datasets.\n",
       "Leveraging your domain data using RAG provides 2 significant benefits:\n",
       "- you don't need to fine-tune your model as often (or at all)\n",
       "- avoid hallucinations\n",
       ".\n",
       "On the ğ—¯ğ—¼ğ˜ ğ˜€ğ—¶ğ—±ğ—², to implement RAG, you have to:\n",
       "3. Embed the user's question using an embedding model (e.g., BERT). Use the embedding to query your vector DB and find the most similar vectors using a distance function (e.g., cos similarity).\n",
       "4. Get the top N closest vectors and their metadata.\n",
       "5. Attach the extracted top N vectors metadata + the chat history to the input prompt.\n",
       "6. Pass the prompt to the LLM.\n",
       "7. Insert the user question + assistant answer to the chat history.\n",
       ".\n",
       "But the question is, ğ—µğ—¼ğ˜„ do you ğ—¸ğ—²ğ—²ğ—½ ğ˜†ğ—¼ğ˜‚ğ—¿ ğ˜ƒğ—²ğ—°ğ˜ğ—¼ğ—¿ ğ——ğ—• ğ˜‚ğ—½ ğ˜ğ—¼ ğ—±ğ—®ğ˜ğ—² ğ˜„ğ—¶ğ˜ğ—µ ğ˜ğ—µğ—² ğ—¹ğ—®ğ˜ğ—²ğ˜€ğ˜ ğ—±ğ—®ğ˜ğ—®?\n",
       "â†³ You need a real-time streaming pipeline.\n",
       "How do you implement it?\n",
       "You need 2 components:\n",
       "â†³ A streaming processing framework. For example, Bytewax is built in Rust for efficiency and exposes a Python interface for ease of use - you don't need Java to implement real-time pipelines anymore.\n",
       "ğŸ”— Bytewax:\n",
       "https://lnkd.in/dbJDDvKB\n",
       "â†³ A vector DB. For example, Qdrant provides a rich set of features and a seamless experience.\n",
       "ğŸ”— Qdrant:\n",
       "https://qdrant.tech/\n",
       ".\n",
       "Here is an example of how to implement a streaming pipeline for financial news â†“\n",
       "ğŸ­. Financial news data source (e.g., Alpaca):\n",
       "To populate your vector DB, you need a historical API (e.g., RESTful API) to add data to your vector DB in batch mode between a desired [start_date, end_date] range. You can tweak the number of workers to parallelize this step as much as possible.\n",
       "â†’ You run this once in the beginning.\n",
       "You need the data exposed under a web socket to ingest news in real time. So, you'll be able to listen to the news and ingest it in your vector DB as soon as they are available.\n",
       "â†’ Listens 24/7 for financial news.\n",
       "ğŸ®. Build the streaming pipeline using Bytewax:\n",
       "Implement 2 input connectors for the 2 different types of APIs: RESTful API & web socket.\n",
       "The rest of the steps can be shared between both connectors â†“\n",
       "- Clean financial news documents.\n",
       "- Chunk the documents.\n",
       "- Embed the documents (e.g., using Bert).\n",
       "- Insert the embedded documents + their metadata to the vector DB (e.g., Qdrant).\n",
       "ğŸ¯-ğŸ³. When the users ask a financial question, you can leverage RAG with an up-to-date vector DB to search for the latest news in the industry.\n",
       "hashtag\n",
       "#\n",
       "machinelearning\n",
       "hashtag\n",
       "#\n",
       "mlops\n",
       "hashtag\n",
       "#\n",
       "deeplearning \"</p>\n",
       "            \n",
       "        </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_question = \"Post about Qdrant\"\n",
    "retrieved_results = vectordb_retriever.search(query_question,limit=3,return_all=True)\n",
    "for post in retrieved_results['posts']:\n",
    "    vectordb_retriever.render_as_html(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/paul.json', 'data/data.json', 'data/fetched_data.json']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\"data/\"+p for p in os.listdir(\"data\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
