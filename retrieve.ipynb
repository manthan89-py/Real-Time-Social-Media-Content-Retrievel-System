{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manthan/Drive/Work/VSCode/LLMs/Real Time Retrievel System for Social Media/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils.embedding import EmbeddingModelSingleton, CrossEncoderModelSingleton\n",
    "from utils.qdrant import build_qdrant_client\n",
    "from utils.retriever import QdrantVectorDBRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "embedding_model = EmbeddingModelSingleton()\n",
    "cross_encode_model = CrossEncoderModelSingleton()\n",
    "qdrant_client = build_qdrant_client()\n",
    "\n",
    "vectordb_retriever = QdrantVectorDBRetriever(\n",
    "    embedding_model=embedding_model,\n",
    "    cross_encoder_model=cross_encode_model,\n",
    "    vector_db_client=qdrant_client\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "all_posts = vectordb_retriever.scroll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"font-family: Arial, sans-serif; color: black; margin: 10px; padding: 20px; border-radius: 10px; background-color: #f3f3f3; box-shadow: 0 0 10px rgba(0,0,0,0.1);\">\n",
       "            <h2 style=\"color: #333;\">Post ID: Post_16</h2>\n",
       "            <h3 style=\"color: #555;\">Chunk ID: 7a2c935a-b89f-61b6-76cb-151a3ae8e068</h3>\n",
       "            <p><strong>Embedding Length:</strong> 384</p>\n",
       "        <p><strong>Score:</strong> 0.70371795</p><p><strong>Rerank Score:</strong> -0.1416376531124115</p>\n",
       "            <p><strong>Chunked Text:</strong> \" real - time data source, bytewax to transform raw text into vector embeddings, and qdrant as a serverless vector db, to store and retrieve embeddings at inference time. you will find the link to the github repo in the comment section below what's next? in the next lecture, we will cover the inference pipeline, so our system can serve predictions to end users. thank you paul iusztin and alexandru rzvan for making this course possible. it is a pleasure working with you guys! - - - - hi there! it's pau labarta bajo every week i share free, hands - on content, on production - grade ml, to help you build real - world ml products. follow me and click on the so you don't miss what's coming next hashtag # machinelearning hashtag # mlops hashtag # realworldml hashtag # llmops hashtag # llms hashtag # bytewax hashtag # qdrant hashtag # vectordb hashtag # realtimeml \"</p>\n",
       "            <p><strong>Full Raw Text:</strong> \" 🎬 𝗟𝗲𝘀𝘀𝗼𝗻 𝟯 of the 𝗛𝗮𝗻𝗱𝘀-𝗼𝗻 𝗟𝗟𝗠 𝗰𝗼𝘂𝗿𝘀𝗲 is 𝗢𝗨𝗧 🎬\n",
       "𝗖𝗼𝗻𝘁𝗲𝘅𝘁\n",
       "LLMs are as good as the data you embed in your prompts. And for many real-world problems, this means the data needs to be both\n",
       "→ good, and\n",
       "→ fresh\n",
       "𝗘𝘅𝗮𝗺𝗽𝗹𝗲 💁\n",
       "Imagine you build a great LLM-based financial advisor… but you only feed it with outdated data.\n",
       "No matter how good your model is, the predictions it will generate will be rubbish 🫣\n",
       "So, the question is\n",
       "𝙃𝙤𝙬 𝙙𝙤 𝙛𝙚𝙚𝙙 𝙛𝙧𝙚𝙨𝙝 𝙙𝙖𝙩𝙖 𝙩𝙤 𝙮𝙤𝙪𝙧 𝙇𝙇𝙈 ❓\n",
       "𝗧𝗵𝗲 𝘀𝗼𝗹𝘂𝘁𝗶𝗼𝗻 🧠\n",
       "You need to build a 𝗿𝗲𝗮𝗹-𝘁𝗶𝗺𝗲 𝘁𝗲𝘅𝘁 𝗲𝗺𝗯𝗲𝗱𝗱𝗶𝗻𝗴 𝗽𝗶𝗽𝗲𝗹𝗶𝗻𝗲, that\n",
       "→ 𝗜𝗻𝗴𝗲𝘀𝘁𝘀 raw text from your data source, in real-time\n",
       "→ 𝗧𝗿𝗮𝗻𝘀𝗳𝗼𝗿𝗺𝘀 this raw text into vector embeddings, and\n",
       "→ 𝗦𝘁𝗼𝗿𝗲𝘀 these embeddings in a VectorDB, so your LLM can fetch and use them for Retrieval Augmented Generation (RAG) at inference time.\n",
       "𝗙𝘂𝗹𝗹 𝘀𝗼𝘂𝗿𝗰𝗲 𝗰𝗼𝗱𝗲 👨🏻‍💻\n",
       "In Lesson 3 of the 𝗛𝗮𝗻𝗱𝘀-𝗼𝗻 𝗟𝗟𝗠 𝗖𝗼𝘂𝗿𝘀𝗲, you will find a full source code implementation of a text embedding pipeline, that uses:\n",
       "→\n",
       "Alpaca\n",
       "News API as our real-time data source,\n",
       "→\n",
       "Bytewax\n",
       "to transform raw text into vector embeddings, and\n",
       "→\n",
       "Qdrant\n",
       "as a Serverless Vector DB, to store and retrieve embeddings at inference time.\n",
       "You will find the link to the 𝗚𝗶𝘁𝗛𝘂𝗯 𝗿𝗲𝗽𝗼 in the comment section below ⬇️\n",
       "𝗪𝗵𝗮𝘁'𝘀 𝗻𝗲𝘅𝘁? ⏭️\n",
       "In the next lecture, we will cover the 𝗶𝗻𝗳𝗲𝗿𝗲𝗻𝗰𝗲 𝗽𝗶𝗽𝗲𝗹𝗶𝗻𝗲, so our system can serve predictions to end users. 🚀\n",
       "THANK YOU\n",
       "Paul Iusztin\n",
       "and\n",
       "Alexandru Răzvanț 👋\n",
       "for making this course possible. It is a pleasure working with you guys!\n",
       "----\n",
       "Hi there! It's\n",
       "Pau Labarta Bajo\n",
       "👋\n",
       "Every week I share free, hands-on content, on production-grade ML, to help you build real-world ML products.\n",
       "𝗙𝗼𝗹𝗹𝗼𝘄 𝗺𝗲 and 𝗰𝗹𝗶𝗰𝗸 𝗼𝗻 𝘁𝗵𝗲 🔔 so you don't miss what's coming next\n",
       "hashtag\n",
       "#\n",
       "machinelearning\n",
       "hashtag\n",
       "#\n",
       "mlops\n",
       "hashtag\n",
       "#\n",
       "realworldml\n",
       "hashtag\n",
       "#\n",
       "llmops\n",
       "hashtag\n",
       "#\n",
       "llms\n",
       "hashtag\n",
       "#\n",
       "bytewax\n",
       "hashtag\n",
       "#\n",
       "qdrant\n",
       "hashtag\n",
       "#\n",
       "vectordb\n",
       "hashtag\n",
       "#\n",
       "realtimeml \"</p>\n",
       "            \n",
       "        </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"font-family: Arial, sans-serif; color: black; margin: 10px; padding: 20px; border-radius: 10px; background-color: #f3f3f3; box-shadow: 0 0 10px rgba(0,0,0,0.1);\">\n",
       "            <h2 style=\"color: #333;\">Post ID: Post_30</h2>\n",
       "            <h3 style=\"color: #555;\">Chunk ID: 04baa0ee-ae34-6a18-2237-3b4320f114c9</h3>\n",
       "            <p><strong>Embedding Length:</strong> 384</p>\n",
       "        <p><strong>Score:</strong> 0.66816294</p><p><strong>Rerank Score:</strong> -1.2617921829223633</p>\n",
       "            <p><strong>Chunked Text:</strong> \" and real - time financial news ( e. g. alpaca ) - a stream processing engine ( e. g., bytewax - [ url ] ) - an encoder - only model for embedding the documents ( e. g., pick one from ` sentence - transformers ` ) - a vector db ( e. g., qdrant - [ url ] ) how does it work? on the feature pipeline side : 1. using bytewax, you ingest the financial news and clean them 2. you chunk the news documents and embed them 3. you insert the embedding of the docs along with their metadata ( e. g., the initial text, source _ url, etc. ) to qdrant on the inference pipeline side : 4. the user question is embedded ( using the same embedding model ) 5. using this embedding, you extract the top k most similar news documents from qdrant 6. along with the user question, you inject the necessary metadata from the extracted top k documents into the prompt template ( e. g., the text of documents & its source _ url ) 7. you pass the whole prompt to the llm for the \"</p>\n",
       "            <p><strong>Full Raw Text:</strong> \" 𝗥𝗔𝗚: 𝘄𝗵𝗮𝘁 problems does it solve, and 𝗵𝗼𝘄 it's integrated into 𝗟𝗟𝗠-𝗽𝗼𝘄𝗲𝗿𝗲𝗱 𝗮𝗽𝗽𝗹𝗶𝗰𝗮𝘁𝗶𝗼𝗻𝘀?\n",
       "Let's find out ↓\n",
       "RAG is a popular strategy when building LLMs to add external data to your prompt.\n",
       "=== 𝗣𝗿𝗼𝗯𝗹𝗲𝗺 ===\n",
       "Working with LLMs has 3 main issues:\n",
       "1. The world moves fast\n",
       "An LLM learns an internal knowledge base. However, the issue is that its knowledge is limited to its training dataset.\n",
       "The world moves fast. New data flows on the internet every second. Thus, the model's knowledge base can quickly become obsolete.\n",
       "One solution is to fine-tune the model every minute or day...\n",
       "If you have some billions to spend around, go for it.\n",
       "2. Hallucinations\n",
       "An LLM  is full of testosterone and likes to be blindly confident.\n",
       "Even if the answer looks 100% legit, you can never fully trust it.\n",
       "3. Lack of reference links\n",
       "It is hard to trust the response of the LLM if we can't see the source of its decisions.\n",
       "Especially for important decisions (e.g., health, financials)\n",
       "=== 𝗦𝗼𝗹𝘂𝘁𝗶𝗼𝗻 ===\n",
       "→ Surprize! It is RAG.\n",
       "1. Avoid fine-tuning\n",
       "Using RAG, you use the LLM as a reasoning engine and the external knowledge base as the main memory (e.g., vector DB).\n",
       "The memory is volatile, so you can quickly introduce or remove data.\n",
       "2. Avoid hallucinations\n",
       "By forcing the LLM to answer solely based on the given context, the LLM will provide an answer as follows:\n",
       "-  use the external data to respond to the user's question if it contains the necessary insights\n",
       "- \"I don't know\" if not\n",
       "3. Add reference links\n",
       "Using RAG, you can easily track the source of the data and highlight it to the user.\n",
       "=== 𝗛𝗼𝘄 𝗱𝗼𝗲𝘀 𝗥𝗔𝗚 𝘄𝗼𝗿𝗸? ===\n",
       "Let's say we want to use RAG to build a financial assistant.\n",
       "𝘞𝘩𝘢𝘵 𝘥𝘰 𝘸𝘦 𝘯𝘦𝘦𝘥?\n",
       "- a data source with historical and real-time financial news (e.g. Alpaca)\n",
       "- a stream processing engine (e.g., Bytewax - 🔗\n",
       "https://lnkd.in/dWJytkZ5\n",
       ")\n",
       "- an encoder-only model for embedding the documents (e.g., pick one from `sentence-transformers`)\n",
       "- a vector DB (e.g., Qdrant - 🔗\n",
       "https://lnkd.in/d_FA9Bb3\n",
       ")\n",
       "𝘏𝘰𝘸 𝘥𝘰𝘦𝘴 𝘪𝘵 𝘸𝘰𝘳𝘬?\n",
       "↳ On the feature pipeline side:\n",
       "1. using Bytewax, you ingest the financial news and clean them\n",
       "2. you chunk the news documents and embed them\n",
       "3. you insert the embedding of the docs along with their metadata (e.g., the initial text, source_url, etc.) to Qdrant\n",
       "↳ On the inference pipeline side:\n",
       "4. the user question is embedded (using the same embedding model)\n",
       "5. using this embedding, you extract the top K most similar news documents from Qdrant\n",
       "6. along with the user question, you inject the necessary metadata from the extracted top K documents into the prompt template (e.g., the text of documents & its source_url)\n",
       "7. you pass the whole prompt to the LLM for the final answer\n",
       "hashtag\n",
       "#\n",
       "machinelearning\n",
       "hashtag\n",
       "#\n",
       "mlops\n",
       "hashtag\n",
       "#\n",
       "datascience \"</p>\n",
       "            \n",
       "        </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"font-family: Arial, sans-serif; color: black; margin: 10px; padding: 20px; border-radius: 10px; background-color: #f3f3f3; box-shadow: 0 0 10px rgba(0,0,0,0.1);\">\n",
       "            <h2 style=\"color: #333;\">Post ID: Post_36</h2>\n",
       "            <h3 style=\"color: #555;\">Chunk ID: 6119e826-c23b-0181-8cf3-016680db83e5</h3>\n",
       "            <p><strong>Embedding Length:</strong> 384</p>\n",
       "        <p><strong>Score:</strong> 0.6675552</p><p><strong>Rerank Score:</strong> -2.5309348106384277</p>\n",
       "            <p><strong>Chunked Text:</strong> \" to the llm. 7. insert the user question + assistant answer to the chat history.. but the question is, how do you keep your vector db up to date with the latest data? you need a real - time streaming pipeline. how do you implement it? you need 2 components : a streaming processing framework. for example, bytewax is built in rust for efficiency and exposes a python interface for ease of use - you don't need java to implement real - time pipelines anymore. bytewax : [ url ] a vector db. for example, qdrant provides a rich set of features and a seamless experience. qdrant : [ url ]. here is an example of how to implement a streaming pipeline for financial news 1. financial news data source ( e. g., alpaca ) : to populate your vector db, you need a historical api ( e. g., restful api ) to add data to your vector db in batch mode between a desired [ start _ date, end _ date ] range. you can tweak the number of workers to parallelize this step as much as possible. you run this once in the beginning. you need the data exposed under a web socket to \"</p>\n",
       "            <p><strong>Full Raw Text:</strong> \" To successfully use 𝗥𝗔𝗚 in your 𝗟𝗟𝗠 𝗮𝗽𝗽𝗹𝗶𝗰𝗮𝘁𝗶𝗼𝗻𝘀, your 𝘃𝗲𝗰𝘁𝗼𝗿 𝗗𝗕 must constantly be updated with the latest data.\n",
       "Here is how you can implement a 𝘀𝘁𝗿𝗲𝗮𝗺𝗶𝗻𝗴 𝗽𝗶𝗽𝗲𝗹𝗶𝗻𝗲 to keep your vector DB in sync with your datasets ↓\n",
       ".\n",
       "𝗥𝗔𝗚 is a popular strategy when building LLMs to add context to your prompt about your private datasets.\n",
       "Leveraging your domain data using RAG provides 2 significant benefits:\n",
       "- you don't need to fine-tune your model as often (or at all)\n",
       "- avoid hallucinations\n",
       ".\n",
       "On the 𝗯𝗼𝘁 𝘀𝗶𝗱𝗲, to implement RAG, you have to:\n",
       "3. Embed the user's question using an embedding model (e.g., BERT). Use the embedding to query your vector DB and find the most similar vectors using a distance function (e.g., cos similarity).\n",
       "4. Get the top N closest vectors and their metadata.\n",
       "5. Attach the extracted top N vectors metadata + the chat history to the input prompt.\n",
       "6. Pass the prompt to the LLM.\n",
       "7. Insert the user question + assistant answer to the chat history.\n",
       ".\n",
       "But the question is, 𝗵𝗼𝘄 do you 𝗸𝗲𝗲𝗽 𝘆𝗼𝘂𝗿 𝘃𝗲𝗰𝘁𝗼𝗿 𝗗𝗕 𝘂𝗽 𝘁𝗼 𝗱𝗮𝘁𝗲 𝘄𝗶𝘁𝗵 𝘁𝗵𝗲 𝗹𝗮𝘁𝗲𝘀𝘁 𝗱𝗮𝘁𝗮?\n",
       "↳ You need a real-time streaming pipeline.\n",
       "How do you implement it?\n",
       "You need 2 components:\n",
       "↳ A streaming processing framework. For example, Bytewax is built in Rust for efficiency and exposes a Python interface for ease of use - you don't need Java to implement real-time pipelines anymore.\n",
       "🔗 Bytewax:\n",
       "https://lnkd.in/dbJDDvKB\n",
       "↳ A vector DB. For example, Qdrant provides a rich set of features and a seamless experience.\n",
       "🔗 Qdrant:\n",
       "https://qdrant.tech/\n",
       ".\n",
       "Here is an example of how to implement a streaming pipeline for financial news ↓\n",
       "𝟭. Financial news data source (e.g., Alpaca):\n",
       "To populate your vector DB, you need a historical API (e.g., RESTful API) to add data to your vector DB in batch mode between a desired [start_date, end_date] range. You can tweak the number of workers to parallelize this step as much as possible.\n",
       "→ You run this once in the beginning.\n",
       "You need the data exposed under a web socket to ingest news in real time. So, you'll be able to listen to the news and ingest it in your vector DB as soon as they are available.\n",
       "→ Listens 24/7 for financial news.\n",
       "𝟮. Build the streaming pipeline using Bytewax:\n",
       "Implement 2 input connectors for the 2 different types of APIs: RESTful API & web socket.\n",
       "The rest of the steps can be shared between both connectors ↓\n",
       "- Clean financial news documents.\n",
       "- Chunk the documents.\n",
       "- Embed the documents (e.g., using Bert).\n",
       "- Insert the embedded documents + their metadata to the vector DB (e.g., Qdrant).\n",
       "𝟯-𝟳. When the users ask a financial question, you can leverage RAG with an up-to-date vector DB to search for the latest news in the industry.\n",
       "hashtag\n",
       "#\n",
       "machinelearning\n",
       "hashtag\n",
       "#\n",
       "mlops\n",
       "hashtag\n",
       "#\n",
       "deeplearning \"</p>\n",
       "            \n",
       "        </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_question = \"Post about Qdrant\"\n",
    "retrieved_results = vectordb_retriever.search(query_question,limit=3,return_all=True)\n",
    "for post in retrieved_results['posts']:\n",
    "    vectordb_retriever.render_as_html(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/paul.json', 'data/data.json', 'data/fetched_data.json']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\"data/\"+p for p in os.listdir(\"data\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
